<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>神经辐射场（二）</title>
      <link href="/2024/06/22/%E6%9D%A8%E4%B8%BD%E5%A8%9C/"/>
      <url>/2024/06/22/%E6%9D%A8%E4%B8%BD%E5%A8%9C/</url>
      
        <content type="html"><![CDATA[<p>神经辐射场（Neural Radiance Fields，简称NeRF）是一种计算机视觉技术，用于生成高质量的三维重建模型。它利用深度学习技术从多个视角的图像中提取出对象的几何形状和纹理信息，然后使用这些信息生成一个连续的三维辐射场，从而可以在任意角度和距离下呈现出高度逼真的三维模型。NeRF技术在计算机图形学、虚拟现实、增强现实等领域有着广泛的应用前景。</p><h3 id="小组成员："><a href="#小组成员：" class="headerlink" title="小组成员："></a>小组成员：</h3><p>杨丽娜 2030330038</p><h2 id="课题进度"><a href="#课题进度" class="headerlink" title="课题进度"></a>课题进度</h2><h3 id="第一周："><a href="#第一周：" class="headerlink" title="第一周："></a>第一周：</h3><p>学习相关知识</p><h3 id="第二周："><a href="#第二周：" class="headerlink" title="第二周："></a>第二周：</h3><p>安装anaconda3 创建虚拟环境</p><h3 id="第三周："><a href="#第三周：" class="headerlink" title="第三周："></a>第三周：</h3><p>安装源包</p><h3 id="第四周："><a href="#第四周：" class="headerlink" title="第四周："></a>第四周：</h3><p>安装CUDA和pytorch</p><h3 id="第五周："><a href="#第五周：" class="headerlink" title="第五周："></a>第五周：</h3><p>安装库 准备数据集</p><h3 id="第六周："><a href="#第六周：" class="headerlink" title="第六周："></a>第六周：</h3><p>训练数据</p><h3 id="第七周："><a href="#第七周：" class="headerlink" title="第七周："></a>第七周：</h3><p>使用预训练模型</p><h3 id="第八周："><a href="#第八周：" class="headerlink" title="第八周："></a>第八周：</h3><p>写实验报告</p><h3 id="第九周："><a href="#第九周：" class="headerlink" title="第九周："></a>第九周：</h3><p>写实验报告</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>胸部x射线噪声分类</title>
      <link href="/2024/04/10/%E5%88%98%E6%98%8C%E5%90%AB/"/>
      <url>/2024/04/10/%E5%88%98%E6%98%8C%E5%90%AB/</url>
      
        <content type="html"><![CDATA[<p>深度学习方法在医学成像问题中显示出出色的分类准确性，这在很大程度上归功于大规模数据集的可用性，这些数据集手动标注了干净的标签。然而，考虑到这种人工标注的高成本，新的医学成像分类问题可能需要依赖于从放射学报告中提取的机器生成的噪声标签。事实上，许多胸部x射线(CXR)分类器已经从带有噪声标签的数据集建模，但它们的训练过程通常对噪声标签样本不具有鲁棒性，导致次优模型。此外，CXR数据集大多是多标签的，因此目前的多类噪声标签学习方法不容易适应。</p><p>本课题提出了一种设计用于噪声多标签CXR学习的新方法，该方法从数据集中检测并平滑地重新标记噪声样本，以用于常见多标签分类器的训练。该方法优化了一组多标签描述符(BoMD)，以提高它们与多标签图像注释语言模型产生的语义描述符的相似度。在有噪声多标签训练集和干净测试集上的实验表明，这个模型在许多CXR多标签分类基准中具有最先进的准确性和鲁棒性，包括我们提出的用于系统评估有噪声多标签方法的新基准。</p><h3 id="小组成员："><a href="#小组成员：" class="headerlink" title="小组成员："></a>小组成员：</h3><p>刘昌晗 2130330007</p><h3 id="第一周："><a href="#第一周：" class="headerlink" title="第一周："></a>第一周：</h3><p>通读论文</p><h3 id="第二周："><a href="#第二周：" class="headerlink" title="第二周："></a>第二周：</h3><p>研究文章中提出的方法</p><h3 id="第三周："><a href="#第三周：" class="headerlink" title="第三周："></a>第三周：</h3><p>下载代码和数据集</p><h3 id="第四周："><a href="#第四周：" class="headerlink" title="第四周："></a>第四周：</h3><p>调试数据集的导入和参数设置</p><h3 id="第五周："><a href="#第五周：" class="headerlink" title="第五周："></a>第五周：</h3><p>解决模型训练中遇到的问题</p><h3 id="第六周："><a href="#第六周：" class="headerlink" title="第六周："></a>第六周：</h3><p>模型训练完成并得出分类结果</p><h3 id="第七周："><a href="#第七周：" class="headerlink" title="第七周："></a>第七周：</h3><p>从代码层面研究MID阶段的实现逻辑</p><h3 id="第八周："><a href="#第八周：" class="headerlink" title="第八周："></a>第八周：</h3><p>从代码层面研究NSD阶段的实现逻辑</p><h3 id="第九周："><a href="#第九周：" class="headerlink" title="第九周："></a>第九周：</h3><p>汇报课题完毕</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>视频异常检测新方法MIST研究</title>
      <link href="/2024/04/10/%E8%B5%B5%E8%8B%97%E8%8B%97/"/>
      <url>/2024/04/10/%E8%B5%B5%E8%8B%97%E8%8B%97/</url>
      
        <content type="html"><![CDATA[<p>在MIST框架中，多实例伪标签生成器扮演着至关重要的角色。它负责生成伪标签，这些标签在模型训练过程中起到引导作用，帮助特征编码器E更好地学习和理解数据</p><p>训练多实例伪标签生成器G的过程是整个模型训练的先行步骤。通过这一步骤，生成器G能够学习到如何根据输入数据生成对应的伪标签，为后续的特征编码器E的训练打下基础。</p><p>在多实例伪标签生成器G训练完成后，接下来就是特征编码器E的微调过程。在这个过程中，E会利用G生成的伪标签进行学习，不断优化自身的参数，以提高对数据的理解和处理能力。</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员:"></a>小组成员:</h3><p>赵苗苗 2130330042</p><p>黄宝仪 2130330025</p><p>李姣  2130330030</p><h3 id="课题汇报："><a href="#课题汇报：" class="headerlink" title="课题汇报："></a>课题汇报：</h3><p>基于MIST框架的视频异常检测实验报告</p><p>摘要：本文提出了一种多实例自训练框架（MIST），旨在解决弱监督视频异常检测（WS-VAD）问题。该框架由多实例伪标签生成器和自引导注意力增强特征编码器组成，通过自训练方案优化这两个组件，最终获得任务特定的特征编码器。在两个公开数据集上的实验结果表明，该方法是有效的，并且与现有的监督和弱监督方法相比具有竞争力，特别是在上海科技数据集上获得了94.83%的帧级AUC。</p><p>关键词：视频异常检测；多实例学习；自训练；注意力机制</p><p>一、引言</p><p>视频异常检测是智能监控系统中的一个重要任务，旨在检测视频中的异常事件。然而，由于异常事件的罕见性和多样性，开发一种能够准确检测异常的模型仍然具有挑战性。本文旨在解决WS-VAD问题，提出了一种多实例自训练框架（MIST），该框架由多实例伪标签生成器和自引导注意力增强特征编码器组成，通过自训练方案优化这两个组件，最终获得任务特定的特征编码器。</p><p>二、相关工作</p><ol><li><p>弱监督视频异常检测：WS-VAD是一种基于视频级标注的异常检测方法，旨在区分正常事件和异常事件。现有的WS-VAD方法可以分为两类：编码器无关方法和编码器相关方法。编码器无关方法使用任务无关的视频特征来估计异常分数，而编码器相关方法则同时训练特征编码器和分类器。</p></li><li><p>多实例学习：MIL是一种流行的弱监督学习方法，用于处理视频相关任务。MIL将视频视为一个包，将视频中的剪辑视为实例，并使用特定的特征&#x2F;分数聚合函数来间接监督实例级学习。</p></li><li><p>自训练：自训练是一种在半监督学习中广泛研究的方法，通过在未标记数据上生成伪标签来增加标记数据，以利用标记和未标记数据的信息。最近的深度自训练涉及特征编码器的表示学习和分类器的细化，主要应用于半监督学习和领域适应。</p></li></ol><p>三、方法</p><ol><li><p>概述：给定一个带有N个剪辑的视频V&#x3D;{vi}N i&#x3D;1，标注的视频级标签Y∈{1,0}表示该视频中是否存在异常事件。我们将视频V视为一个包，将视频中的剪辑vi视为实例。具体来说，一个负包（即Y&#x3D;0）标记为Bn&#x3D;{vn i}N i&#x3D;1，没有异常实例，而一个正包（即Y&#x3D;1）表示为Ba&#x3D;{va i }N i&#x3D;1，至少有一个异常实例。</p></li><li><p>多实例伪标签生成器：我们引入了一个基于MLP的结构作为伪标签生成器，在MIL范式下进行训练，以生成用于特征编码器ESGA细化的伪标签。</p></li><li><p>自引导注意力增强特征编码器：我们提出了一种自引导注意力模块（SGA），该模块通过伪标签监督优化注意力图生成，以增强任务特定表示的学习。</p></li><li><p>优化过程：我们使用深度MIL排名损失来优化伪标签生成器的学习，并使用交叉熵损失来训练特征编码器ESGA。</p></li></ol><p>四、实验</p><ol><li><p>数据集和指标：我们在两个大型数据集上进行实验，即UCF-Crime和ShanghaiTech，使用两个特征编码器，即C3D和I3D。我们使用帧级接收器操作特征（ROC）曲线下的面积（AUC）作为主要指标，较大的AUC表示更高的区分能力。</p></li><li><p>实现细节：我们使用Adagrad优化器训练伪标签生成器，学习率为0.01。在微调时，我们使用Adam优化器，学习率为1e-4，权重衰减为0.0005，训练300个epoch。</p></li><li><p>比较与相关方法：我们将MIST与相关的最先进的在线方法在准确性和鲁棒性方面进行了比较。结果表明，MIST在所有评估指标上都优于或与其他方法相似，证实了MIST的有效性。</p></li><li><p>任务特定特征编码器：为了验证我们的特征编码器能够产生促进其他编码器无关方法的任务特定表示，我们还使用I3D进行了相关实验。结果表明，使用我们的MIST微调特征后，所有编码器无关方法的结果都得到了提升，显示出域差距的减小。</p></li><li><p>消融研究：我们进行了消融研究，分析了生成的伪标签（PLs）、自引导注意力模块（SGA）和分类器头Hg在特征编码器ESGA中的影响。结果表明，生成的伪标签对MIST的性能有显著影响，SGA增强了特征编码器在强调信息区域和区分异常事件与正常事件方面的能力。</p></li><li><p>可视化结果：我们可视化了模型的时间预测，结果表明我们的模型能够准确地定位异常事件，并在正常视频上预测出非常接近零的异常分数，显示了我们模型的有效性和鲁棒性。</p></li></ol><p>五、结论</p><p>在本文中，我们提出了一种多实例自训练框架（MIST），用于有效地细化任务特定的判别表示，仅使用视频级注释。特别是，MIST由1）一个多实例伪标签生成器组成，该生成器采用稀疏连续采样策略来生成更可靠的剪辑级伪标签，以及2）一个自引导注意力增强特征编码器，旨在在提取任务特定表示的同时自动关注帧中的异常区域。此外，我们采用了一种自训练方案来优化这两个组件，并最终获得了一个任务特定的特征编码器。在两个公共数据集上的广泛实验证明了我们方法的有效性，我们的方法与现有的监督和弱监督方法相比具有竞争力，特别是在上海科技数据集上获得了94.83%的帧级AUC。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>基于VR设备的眼动数据采集</title>
      <link href="/2024/04/10/%E9%BB%84%E8%8C%9C%E5%A6%8D/"/>
      <url>/2024/04/10/%E9%BB%84%E8%8C%9C%E5%A6%8D/</url>
      
        <content type="html"><![CDATA[<p>本网站专注于呈现基于 VR 设备采集的眼动数据。通过先进技术，精准记录眼动轨迹，为研究与分析提供丰富资源。</p><h3 id="小组成员："><a href="#小组成员：" class="headerlink" title="小组成员："></a>小组成员：</h3><p>黄茜妍 2130330026</p><p>姜淼  2130330029</p><h3 id="第一周："><a href="#第一周：" class="headerlink" title="第一周："></a>第一周：</h3><p>学习有关知识</p><h3 id="第二周："><a href="#第二周：" class="headerlink" title="第二周："></a>第二周：</h3><p>安装node.js和hexo，注册github</p><h3 id="第三周："><a href="#第三周：" class="headerlink" title="第三周："></a>第三周：</h3><p>初始化博客项目</p><h3 id="第四周："><a href="#第四周：" class="headerlink" title="第四周："></a>第四周：</h3><p>安装依赖，本地运行博客</p><h3 id="第五周："><a href="#第五周：" class="headerlink" title="第五周："></a>第五周：</h3><p>配置github</p><h3 id="第六周："><a href="#第六周：" class="headerlink" title="第六周："></a>第六周：</h3><p>配置hexo</p><h3 id="第七周："><a href="#第七周：" class="headerlink" title="第七周："></a>第七周：</h3><p>完善博客界面</p><h3 id="第八周："><a href="#第八周：" class="headerlink" title="第八周："></a>第八周：</h3><p>部署到github</p><h3 id="第九周："><a href="#第九周：" class="headerlink" title="第九周："></a>第九周：</h3><p>汇报课题完毕</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>用于弱光图像增强的零参考深度曲线估计</title>
      <link href="/2024/04/10/%E5%88%98%E4%BD%B3%E6%AC%A3/"/>
      <url>/2024/04/10/%E5%88%98%E4%BD%B3%E6%AC%A3/</url>
      
        <content type="html"><![CDATA[<p>课题介绍</p><p>本文提出了一个零参考深度曲线估计，它可以使用零参考图像进行端到端训练，这是通过将弱光图像增强任务公式化为图像特定的曲线估计问题并设计一组可微分的非参考损失来实现的，（图像作为输入，曲线作为输出）从而获得增强图像。另外，通过训练一个轻量级的网络（DCE-NET），来预测一个像素级的、高阶的曲线，并通过该曲线来调整图像。</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员"></a>小组成员</h3><p>刘佳欣 2130330033</p><p>陆彩云 2130330036</p><p>刘文雅  2130330034</p><h2 id="进度汇报"><a href="#进度汇报" class="headerlink" title="进度汇报"></a>进度汇报</h2><h3 id="第一周"><a href="#第一周" class="headerlink" title="第一周:"></a>第一周:</h3><p>1.小组成员学习 python 和 pytorch 基础，更深入理解用于弱光图像增强的零参考深度曲线估计，把握项目的大致背景</p><p>2.小组成员已安装好软件和配置好其相关环境，观看了相关的学习视频</p><p>3.小组成员对论文进行了初步的翻译阅读和理解，并进行了部分批注，代码部分初步开始对应阅读和理解</p><p>汇报人:陆彩云</p><h3 id="第二周"><a href="#第二周" class="headerlink" title="第二周:"></a>第二周:</h3><p>1.小组成员继续学习 python 和 pytorch 基础</p><p>2.观看相关的学习视频</p><p>3.小组成员对论文进行理解批注，代码部分初步开始对应阅读和理解</p><p>汇报人:刘文雅</p><h3 id="第三周"><a href="#第三周" class="headerlink" title="第三周:"></a>第三周:</h3><p>1.小组成员继续学习 python 和 pytorch 基础</p><p>2.观看相关的学习视频</p><p>3.小组成员对论文的理解批注进行探讨，已将代码部分逐一与论文内容匹配</p><p>4.再次检查环境是否符合代码运行，开始运行代码</p><p>汇报人:刘佳欣</p><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周:"></a>第四周:</h3><p>1.小组成员开始学习论文和相应代码</p><p>2该代码以弱光图像作为输入，并产生高阶曲线作为其输出</p><p>3.已跑通lowlight._test.py与lowlight_train.py，已观察test中result图片与原图对比</p><p>汇报人:刘佳欣</p><h3 id="第五周"><a href="#第五周" class="headerlink" title="第五周:"></a>第五周:</h3><p>1.小组成员学习论文和对应代码</p><p>2.了解模型架构，学习数据的处理与变换，掌握模型的输入输出</p><p>3.讨论研究一条替换LE曲线使其弱光图像增强效果更好的曲线</p><p>汇报人:陆彩云</p><h3 id="第六周"><a href="#第六周" class="headerlink" title="第六周:"></a>第六周:</h3><p>1.小组成员学习论文和对应代码</p><p>2.学习Visio绘图相关知识</p><p>3.深入学习模型架构，掌握模型的输入输出</p><p>4.讨论研究一条替换LE曲线使其弱光图像增强效果更好的曲线，现已讨论但该函数存在不足，不能在有效范围内迭代四次，仍需加强</p><p>汇报人：刘文雅</p><h3 id="第七周"><a href="#第七周" class="headerlink" title="第七周:"></a>第七周:</h3><p>1.小组成员学习论文和对应代码</p><p>2.学习Visio绘图相关知识</p><p>3.已找出替换LE曲线的相关函数：f(x;α)&#x3D;ln(αx+1)&#x2F;ln(α+1)</p><p>4.正在进行：attention自注意力机制训练a再乘上b常量得到想要的α值</p><p>汇报人：刘佳欣</p><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周:"></a>第八周:</h3><p>1.利用Visio绘图工具进行课堂进程展示</p><p>2.正在进行：只控制α，α&#x3D;a×b，attention自注意力机制训练a再乘上b常量（PSNR值升到20，调试看效果）得到想要的α值，控制a在0到1之间，用channel attention对32个channel每个进行打分，α通过softmax函数、attention自注意力机制学习，找出了一个可以运行并跑通的代码，验证图片确实增强并可以使用，求出PSNR、SSIM、MAE等数值和原来数据进行对比</p><p>3.撰写实验报告</p><h3 id="第九周"><a href="#第九周" class="headerlink" title="第九周:"></a>第九周:</h3><p>汇报人：刘佳欣、陆彩云、刘文雅</p><p>汇报课题完毕</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>智能机器人的面部表情识别之嘴部表情的生成</title>
      <link href="/2024/04/09/%E8%B4%BA%E7%88%BD/"/>
      <url>/2024/04/09/%E8%B4%BA%E7%88%BD/</url>
      
        <content type="html"><![CDATA[<p>智能机器人的面部表情识别应用广泛,我们小组以生成人脸嘴部表情作为案例研究，任务是生成特定表情的图片数据。这个任务是基于PyTorch 实现一个DCGAN(Deep Conxglutignal GAN)模型,用于生成人脸嘴部表情的逼真图像。DCGAN是一种生成对抗网络(GAN)的变体，通过卷积神经网络结构来实现图像的生成。基于 Rxterch 框架，搭建 DCGAN生成器与判别器模型。生成器网络的输入是一个随机向量,这个随机向量是生成器网络的输入，用于生成逼真的人脸嘴部表情图像。生成器网络的输出是一个合成的图像，这个图像应当具有逼真的人脸嘴部表情特征。</p><p>判别器网络的输出是一个介于0和1之间的概率值,用于表示输入图像是真实图像的概率。如果输出接近1，则表示判别器认为输入图像是真实的:如果输出接近0，则表示判别器认为输入图像是生成器生成的假图像。</p><h3 id="小组成员："><a href="#小组成员：" class="headerlink" title="小组成员："></a>小组成员：</h3><p>智能 2101 贺爽 2130330023</p><p>智能 2101 谢妍妮 2130330039</p><h2 id="课题进度"><a href="#课题进度" class="headerlink" title="课题进度"></a>课题进度</h2><h3 id="第五周"><a href="#第五周" class="headerlink" title="第五周"></a>第五周</h3><p>1.进一步进行尝试代码的运行，并思考代码运行中出现的问题。<br>2.查阅资料并尝试解决代码运行中的问题。<br>3.进一步学习了解多模态信息融合，逼真度评估等相关内容。</p><p>汇报人:贺爽 谢妍妮</p><h3 id="第六周"><a href="#第六周" class="headerlink" title="第六周"></a>第六周</h3><p>1.进一步进行尝试代码的运行，并思考代码运行中出现的问题。<br>2.查阅资料并尝试解决代码运行中的问题。<br>3.进一步学习了解多模态信息融合，逼真度评估等相关内容。</p><p>汇报人:贺爽 谢妍妮</p><h3 id="第七周"><a href="#第七周" class="headerlink" title="第七周"></a>第七周</h3><p>1.思考代码运行中出现的问题并尝试解决。。<br>2.查阅资料阅读相关论文。。<br>3.加深学习了解逼真度评估等相关内容。</p><p>汇报人:贺爽 谢妍妮</p><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周"></a>第八周</h3><p>1.成功实现代码的运行<br>2.认真研究并理解代码</p><p>汇报人:贺爽 谢妍妮</p><h3 id="第九周"><a href="#第九周" class="headerlink" title="第九周"></a>第九周</h3><p>汇报课题完成。</p><p>汇报人:贺爽 谢妍妮</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>自监督学习</title>
      <link href="/2024/04/01/%E5%8F%B6%E6%A0%BC%E9%91%AB/"/>
      <url>/2024/04/01/%E5%8F%B6%E6%A0%BC%E9%91%AB/</url>
      
        <content type="html"><![CDATA[<p>为监督学习量身定制自监督，提出了一个为监督学习量身定制的辅助自我监督任务，即可定位旋转 （LoRot），它通过仅旋转图像的一部分来形成定位测验，这方法有表征丰富，数据分布小，鲁棒性良好等优点</p><h3 id="小组成员："><a href="#小组成员：" class="headerlink" title="小组成员："></a>小组成员：</h3><p>叶格鑫 2130330015</p><h3 id="第一周："><a href="#第一周：" class="headerlink" title="第一周："></a>第一周：</h3><p>查阅10篇以上相关文献，做笔记。<br>总结文献中的重要观点和研究方向。</p><h3 id="第二周："><a href="#第二周：" class="headerlink" title="第二周："></a>第二周：</h3><p>阅读并详细分析提供的LoRot文档。<br>编写实验方案和详细计划，包括时间安排和资源需求。</p><h3 id="第三周："><a href="#第三周：" class="headerlink" title="第三周："></a>第三周：</h3><p>下载CIFAR-10数据集并进行数据预处理。<br>编写数据预处理的脚本和文档。</p><h3 id="第四周："><a href="#第四周：" class="headerlink" title="第四周："></a>第四周：</h3><p>搭建实验框架，编写并调试LoRot模型代码。<br>完成初步测试，修正发现的技术问题。</p><h3 id="第五周："><a href="#第五周：" class="headerlink" title="第五周："></a>第五周：</h3><p>运行稳健性测试实验，记录数据。<br>分析初步结果，进行必要的调整。</p><h3 id="第六周："><a href="#第六周：" class="headerlink" title="第六周："></a>第六周：</h3><p>运行泛化能力测试实验，记录数据。<br>与对比方法CSI结合进行测试，记录数据</p><h3 id="第七周："><a href="#第七周：" class="headerlink" title="第七周："></a>第七周：</h3><p>统计和分析所有实验数据。<br>生成图表和表格，进行结果的详细对比和分析。</p><h3 id="第八周："><a href="#第八周：" class="headerlink" title="第八周："></a>第八周：</h3><p>撰写报告各部分内容，注重实验细节和数据解释。<br>确保报告结构合理，内容详实。</p><h3 id="第九周："><a href="#第九周：" class="headerlink" title="第九周："></a>第九周：</h3><p>校对和修订报告，确保文字通顺、逻辑清晰。<br>征求反馈意见，进行最终修改。<br>提交最终版报告。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>多实例异常检测</title>
      <link href="/2024/04/01/%E6%9B%B9%E5%AD%90%E6%80%A1/"/>
      <url>/2024/04/01/%E6%9B%B9%E5%AD%90%E6%80%A1/</url>
      
        <content type="html"><![CDATA[<p>弱监督视频异常检测是一种基于可分特征将异常从正常事件中检测出来的任务。然而，大多数现有工作受限于不充分的特征表示。</p><p>本工作中，作者提出了多示例自训练的框架（MIST），该框架仅使用视频级别标签，能够高效地优化任务相关的特征表示。</p><p>其中，MIST包括了（1）一个多示例学习的伪类标生成器，它采用了稀疏连续采样策略来产生更加可信的伪类标（reliable clip-level pseudo labels）；（2）一个自引导注意力模块增强的特征提取器，用以在特征提取过程中使提取器更关注异常区域。 另外，在实验时作者采用了自训练的方法来优化这两个部件，并最终得到一个任务特定的特征提取器。</p><h3 id="小组成员："><a href="#小组成员：" class="headerlink" title="小组成员："></a>小组成员：</h3><p>曹子怡 2130330017</p><p>王思博 2130330013</p><h3 id="第一周："><a href="#第一周：" class="headerlink" title="第一周："></a>第一周：</h3><p><strong>精通论文内容，了解论文实现的目的以及创新点</strong></p><p><img src="/2024/04/01/%E6%9B%B9%E5%AD%90%E6%80%A1/wps1.jpg" alt="img"> </p><h3 id="第二周："><a href="#第二周：" class="headerlink" title="第二周："></a>第二周：</h3><p><strong>对相关内容进行文献综述，梳理项目的整体流程</strong></p><img src="/2024/04/01/%E6%9B%B9%E5%AD%90%E6%80%A1/wps2.jpg" alt="img"><h3 id="第三周："><a href="#第三周：" class="headerlink" title="第三周："></a>第三周：</h3><p> <strong>读懂代码及每个流程的具体细节处理，进行代码批注。</strong></p><p><img src="/2024/04/01/%E6%9B%B9%E5%AD%90%E6%80%A1/image-20240521215503013.png" alt="image-20240521215503013"></p><h3 id="第四周："><a href="#第四周：" class="headerlink" title="第四周："></a>第四周：</h3><p><strong>跑通论文代码，在ShanghaiTech数据集上获得了94.83％的帧级AUC，与论文作者实验结果一致</strong></p><p><img src="/2024/04/01/%E6%9B%B9%E5%AD%90%E6%80%A1/wps3.png" alt="img"></p><h3 id="第五-八周："><a href="#第五-八周：" class="headerlink" title="第五-八周："></a>第五-八周：</h3><p>在原论文的基础上进行进一步的更改与创新，具体更改点如下：</p><ol><li><p>训练MIL模型的损失函数：</p><p><strong># Define loss functions</strong></p><p><strong>mseloss &#x3D; torch.nn.MSELoss(reduction&#x3D;’mean’)</strong></p><p><strong>mseloss_vector &#x3D; torch.nn.MSELoss(reduction&#x3D;’none’)</strong></p><p><strong>binary_CE_loss &#x3D; torch.nn.BCELoss(reduction&#x3D;’mean’)</strong></p><p><strong>binary_CE_loss_vector &#x3D; torch.nn.BCELoss(reduction&#x3D;’none’)</strong></p><p><strong>def cross_entropy(logits, target, size_average&#x3D;True):</strong></p><p>  <strong>if size_average:</strong></p><p>​    <strong>return torch.mean(torch.sum(-target * F.log_softmax(logits, -1), -1))</strong></p><p>  <strong>else:</strong></p><p>​    <strong>return torch.sum(torch.sum(-target * F.log_softmax(logits, -1), -1))</strong></p><p><strong>def hinger_loss(anomaly_score, normal_score):</strong></p><p>  <strong>return F.relu((1 - anomaly_score + normal_score))</strong></p><p><strong>def normal_smooth(element_logits, labels, device):</strong></p><p>  <strong>normal_smooth_loss &#x3D; torch.zeros(0).to(device)</strong></p><p>  <strong>real_size &#x3D; int(element_logits.shape[0])</strong></p><p>  <strong>for i in range(real_size):</strong></p><p>​    <strong>if labels[i] &#x3D;&#x3D; 0:</strong></p><p>​      <strong>normal_smooth_loss &#x3D; torch.cat((normal_smooth_loss, torch.var(element_logits[i]).unsqueeze(0)))</strong></p><p>  <strong>normal_smooth_loss &#x3D; torch.mean(normal_smooth_loss, dim&#x3D;0)</strong></p><p>  <strong>return normal_smooth_loss</strong></p><p><strong>def KMXMILL_individual(element_logits, seq_len, labels, device, loss_type&#x3D;’CE’, args&#x3D;None):</strong></p><p>  <strong>k &#x3D; np.ceil(seq_len &#x2F; args.k).astype(‘int32’)</strong></p><p>  <strong>instance_logits &#x3D; torch.zeros(0).to(device)</strong></p><p>  <strong>real_label &#x3D; torch.zeros(0).to(device)</strong></p><p>  <strong>real_size &#x3D; int(element_logits.shape[0])</strong></p><p>  <strong>for i in range(real_size):</strong></p><p>​    <strong>tmp, tmp_index &#x3D; torch.topk(element_logits[i][:seq_len[i]], k&#x3D;int(k[i]), dim&#x3D;0)</strong></p><p>​    <strong>instance_logits &#x3D; torch.cat((instance_logits, tmp), dim&#x3D;0)</strong></p><p>​    <strong>if labels[i] &#x3D;&#x3D; 1:</strong></p><p>​      <strong>real_label &#x3D; torch.cat((real_label, torch.ones((int(k[i]), 1)).to(device)), dim&#x3D;0)</strong></p><p>​    <strong>else:</strong></p><p>​      <strong>real_label &#x3D; torch.cat((real_label, torch.zeros((int(k[i]), 1)).to(device)), dim&#x3D;0)</strong></p><p>  <strong>if loss_type &#x3D;&#x3D; ‘CE’:</strong></p><p>​    <strong>milloss &#x3D; binary_CE_loss(input&#x3D;instance_logits, target&#x3D;real_label)</strong></p><p>​    <strong>return milloss</strong></p><p>  <strong>elif loss_type &#x3D;&#x3D; ‘MSE’:</strong></p><p>​    <strong>milloss &#x3D; mseloss(input&#x3D;instance_logits, target&#x3D;real_label)</strong></p><p>​    <strong>return milloss</strong></p><p><strong>def topk_rank_loss(args, y_pred):</strong></p><p>  <strong>topk_pred &#x3D; torch.mean(torch.topk(y_pred.view([args.batch_size * 2, args.part_num * args.part_len]), args.topk, dim&#x3D;-1)[0], dim&#x3D;-1, keepdim&#x3D;False)</strong></p><p>  <strong>nor_max &#x3D; topk_pred[:args.batch_size]</strong></p><p>  <strong>abn_max &#x3D; topk_pred[args.batch_size:]</strong></p><p>  <strong>err &#x3D; 0</strong></p><p>  <strong>for i in range(args.batch_size):</strong></p><p>​    <strong>err +&#x3D; torch.sum(F.relu(1 - abn_max + nor_max[i]))</strong></p><p>  <em><em>err &#x3D; err &#x2F; (args.batch_size) *</em> 2</em>*</p><p>  <strong>abn_pred &#x3D; y_pred[args.batch_size:]</strong></p><p>  <strong>spar_l1 &#x3D; torch.mean(abn_pred)</strong></p><p>  <em><em>smooth_l2 &#x3D; torch.mean((abn_pred[:, :-1] - abn_pred[:, 1:]) *</em> 2)</em>* </p><p>  <strong>loss &#x3D; err + args.lambda_1 * spar_l1 + args.lambda_2 * smooth_l2</strong></p><p>  <strong>return loss</strong></p><p>2.伪标签生成给予阈值调整：</p><p><strong>def Augment_Pseudo_Label_Generate(args, epoch):</strong></p><p>  <strong>if hasattr(args, ‘type’) and args.type in feature_type:</strong></p><p>​    <strong>model &#x3D; Simple_Regressor(args.input_feature_dim).cuda().train()</strong></p><p>  <strong>else:</strong></p><p>​    <strong>default_type &#x3D; ‘default_type’</strong></p><p>​    <strong>if default_type in feature_type:</strong></p><p>​      <strong>model &#x3D; Simple_Regressor(feature_type[default_type][1]).cuda().eval()</strong></p><p>​      <strong>print(“Warning: ‘type’ attribute not found in args object or not in feature_type dictionary. Using default type:”, default_type)</strong></p><p>​    <strong>else:</strong></p><p>​      <strong>raise KeyError(“Default type ‘{}’ not found in feature_type dictionary.”.format(default_type))</strong></p><p>  <strong>test_keys &#x3D; []</strong></p><p>  <strong>for line in open(args.testing_txt).readlines():</strong></p><p>​    <strong>test_keys.append(line.strip().split(‘,’)[0].split(‘.’)[0] + ‘.npy’)</strong></p><p>  <strong>model &#x3D; Simple_Regressor(args.input_feature_dim).cuda().eval()</strong></p><p>  <strong>pretrained_dict &#x3D; torch.load(args.model_path)</strong></p><p>  <strong>model_dict &#x3D; model.state_dict()</strong></p><p>  <strong>pretrained_dict &#x3D; {k: v for k, v in pretrained_dict.items() if k in model_dict}</strong></p><p>  <strong>model_dict.update(pretrained_dict)</strong></p><p>  <strong>model.load_state_dict(model_dict)</strong></p><p>  <strong>scores_dict &#x3D; {}</strong></p><p>  <strong>test_scores_dict &#x3D; {}</strong></p><p>  <strong>with h5py.File(args.feature_rgb_path, ‘r’) as h5:</strong></p><p>​    <strong>keys &#x3D; list(h5.keys())</strong></p><p>​    <strong>with torch.no_grad():</strong></p><p>​      <strong>for key in keys:</strong></p><p>​        <strong>feat &#x3D; h5[key][:]</strong></p><p>​        <strong>feat &#x3D; np.array(feat)</strong></p><p>​        <strong>if args.norm &#x3D;&#x3D; 2:</strong></p><p>​          <strong>feat &#x3D; feat &#x2F; np.linalg.norm(feat, axis&#x3D;-1, keepdims&#x3D;True)</strong></p><p>​        <strong>feat &#x3D; torch.from_numpy(feat).cuda().float()</strong></p><p>​        <strong>if feat.shape[0] &gt; 1000:</strong></p><p>​          <strong>scores &#x3D; []</strong></p><p>​          <strong>length &#x3D; feat.shape[0] &#x2F;&#x2F; 1000 + 1</strong></p><p>​          <strong>for i in range(length):</strong></p><p>​            <strong>if i &#x3D;&#x3D; length - 1:</strong></p><p>​              <strong>f &#x3D; feat[i * 1000:]</strong></p><p>​            <strong>else:</strong></p><p>​              <strong>f &#x3D; feat[i * 1000:(i + 1) * 1000]</strong></p><p>​            <strong>scores.append(model(f).detach().cpu().numpy())</strong></p><p>​          <strong>scores &#x3D; np.vstack(scores)</strong></p><p>​        <strong>else:</strong></p><p>​          <strong>scores &#x3D; model(feat).detach().cpu().numpy()</strong></p><p>​        <strong>scores &#x3D; scores.squeeze()</strong></p><p>​        <strong>if epoch &#x3D;&#x3D; 1:</strong></p><p>​          <strong>if key in test_keys and np.any(scores &#x3D;&#x3D; 1):</strong></p><p>​            <strong>test_scores_dict[key] &#x3D; np.ones_like(scores)</strong></p><p>​          <strong>else:</strong></p><p>​            <strong>test_scores_dict[key] &#x3D; scores</strong></p><p>​        <strong>else:</strong></p><p>​          <strong>scores[scores &lt; 0.6 * epoch] *&#x3D; 0.6 * epoch</strong></p><p>​          <strong>scores[scores &gt; 0.8] &#x3D; 1</strong></p><p>​          <strong>test_scores_dict[key] &#x3D; scores</strong></p><p>​        <strong># Debugging output to ensure keys are being processed</strong></p><p>​        <strong>if key in test_keys:  # Here we use test_keys</strong></p><p>​          <strong>scores_dict[key] &#x3D; scores</strong></p><p>​        <strong>print(f”Processed key: {key}”)</strong></p><p>  <strong>np.save(args.Pseudo_Labels_dir + ‘&#x2F;train_results.npy’, scores_dict)</strong></p><p>  <strong>np.save(args.Pseudo_Labels_dir + ‘&#x2F;test_results.npy’, test_scores_dict)</strong></p><p>  <strong>Augment_Pseudo_Label_Refilement(args)</strong></p></li></ol><h3 id="课题成果："><a href="#课题成果：" class="headerlink" title="课题成果："></a>课题成果：</h3><p>跑通代码，在ShanghaiTech数据集上获得了96.86％的帧级AUC，与原论文相比提高了2.03%</p><p><img src="/2024/04/01/%E6%9B%B9%E5%AD%90%E6%80%A1/image-20240622093548829.png" alt="image-20240622093548829"></p><h3 id="第九周："><a href="#第九周：" class="headerlink" title="第九周："></a>第九周：</h3><p>汇报课题完毕</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>课程网站构建</title>
      <link href="/2024/04/01/%E7%A8%8B%E4%BC%9F%E8%B6%85/"/>
      <url>/2024/04/01/%E7%A8%8B%E4%BC%9F%E8%B6%85/</url>
      
        <content type="html"><![CDATA[<p>Hexo是一个基于Node.js的静态博客生成框架，它允许用户以简介的方式创建、管理和发布博客内容，Hexo的设计初衷是为了让用户能够更轻松地搭建个人博客，无需复杂的数据库或服务器设置，只需生成一组静态HTML文件，然后将其部署到一个静态文件托管服务上，如GitHub pages、Netlify等。而GitHub Pages能够为我们提供一个免费的静态文件托管平台，创建的<a href="http://github.io非常适合用于个人博客、项目文档、作品展示等内容./">http://github.io非常适合用于个人博客、项目文档、作品展示等内容。</a></p><h3 id="小组成员："><a href="#小组成员：" class="headerlink" title="小组成员："></a>小组成员：</h3><p>程伟超 2130330002</p><h3 id="第一周："><a href="#第一周：" class="headerlink" title="第一周："></a>第一周：</h3><ol><li>安装Node.js。<br>这里建议大家不要使用Ubuntu自带的apt下载nodejs，因为版本不一定是最新的，建议直接去Nodejs官网去下载比较新的版本。一般情况下nodejs会附带配套的npm（Node Package Manager，是一个用于Node.js环境的包管理工具），如果没有的话可以单独安装一下npm。</li></ol><h3 id="第二周："><a href="#第二周：" class="headerlink" title="第二周："></a>第二周：</h3><ol start="2"><li>安装Git。<br>相信程序员伙伴们都认识这个工具，它实际上是一个非常强大的版本控制软件。<br>对于还没有使用过git的小伙伴，在Ubuntu下直接apt安装即可（运行以下命令）：<br> sudo apt install git<br>windows系统可以自行STFW，git的安装相对简单。</li></ol><h3 id="第三周："><a href="#第三周：" class="headerlink" title="第三周："></a>第三周：</h3><ol start="3"><li>安装hexo。<br>在命令行中执行：<br> npm install hexo-cli -g<br>等待自动安装完成后，输入：<br> hexo -v<br>检查是否安装成功，若安装成功，应该显示hexo的版本号。<br>本文使用的hexo版本为hexo-cli: 4.3.1</li><li>在github上创建<a href="http://github.io远程仓库./">http://github.io远程仓库。</a><br>在github中新建一个仓库，Repository name设置为<username>.github.io，比如我的github用户名是A-little-star，那么创建的仓库名称应为A-little-star.github.io，注意这里必须使用自己的用户名，否则会出错。</username></li></ol><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><ol><li>初始化Hexo<br>创建一个文件夹用于存放Hexo项目。比如创建一个名为mypage的目录。<br>进入mypage目录，运行：<br> hexo init blog<br>以上命令会在mypage目录下创建一个新的目录blog，并在blog目录下对hexo项目进行初始化。</li><li>接着进入blog目录，运行：<br> cd blog<br> npm install</li></ol><h3 id="第五周："><a href="#第五周：" class="headerlink" title="第五周："></a>第五周：</h3><ol start="3"><li>安装hexo。<br>在命令行中执行：<br> npm install hexo-cli -g<br>等待自动安装完成后，输入：<br> hexo -v<br>检查是否安装成功，若安装成功，应该显示hexo的版本号。<br>本文使用的hexo版本为hexo-cli: 4.3.1</li><li>在github上创建<a href="http://github.io远程仓库./">http://github.io远程仓库。</a><br>   在github中新建一个仓库，Repository name设置为<username>.github.io，比如我的github用户名是A-little-star，那么创建的仓库名称应为A-little-star.github.io，注意这里必须使用自己的用户名，否则会出错。</username></li></ol><h3 id="第六周："><a href="#第六周：" class="headerlink" title="第六周："></a>第六周：</h3><p>将本地项目部署到远程仓库</p><ol><li><p>安装部署工具<br>继续在本地的项目根目录安装部署工具：<br> npm install hexo-deployer-git –save<br>注意：只有以上命令执行成功才能将主页部署到<a href="http://github.io上去!/">http://github.io上去！</a></p></li><li><p>将本地源代码推送至远程<br>由于<a href="http://github.io仓库中已经存放了部署的静态网页，所以我们新建一个分支src，将源码放在这个分支下面，用于后续开发：">http://github.io仓库中已经存放了部署的静态网页，所以我们新建一个分支src，将源码放在这个分支下面，用于后续开发：</a><br> git add .<br> git commit -m ‘First commit.’<br> git checkout -b src<br> git push -u origin src<br>接下来打开网站就可以看到效果了。</p></li><li><p>之后修改<br>之后每次修改后，都可以用hexo clean &amp;&amp; hexo g来生成网页，在本地通过hexo s调试之后，hexo d部署到远程，然后将源码push到远程仓库即可。</p></li></ol><p>汇报人：</p><p>程伟超</p><p>网站地址：</p><p><a href="https://chengweichao123.github.io/">https://chengweichao123.github.io/</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习模型封装</title>
      <link href="/2024/04/01/%E7%8E%8B%E6%95%8F%E5%AE%87/"/>
      <url>/2024/04/01/%E7%8E%8B%E6%95%8F%E5%AE%87/</url>
      
        <content type="html"><![CDATA[<h3 id="课程介绍"><a href="#课程介绍" class="headerlink" title="课程介绍"></a>课程介绍</h3><p>深度学习模型封装就是将模型的输入和输出格式化，以便在不同的应用场景中使用。在本次课程设计中，我们使用了Pytorch，它是一款流行的深度学习框架，它提供了徐福哦高级的工具和接口，可以轻松地训练、评估和部署深度学习模型。此外，我们还使用了WindowsML来开发智能应用程序，使其能够更加快捷方便运行。</p><h3 id="小组成员："><a href="#小组成员：" class="headerlink" title="小组成员："></a>小组成员：</h3><p>王敏宇 2130330012</p><p>陈艳萍 2130330018</p><h3 id="第一周："><a href="#第一周：" class="headerlink" title="第一周："></a>第一周：</h3><p>理解什么是机器学习模型以及卷积神经网络（CNN）</p><h3 id="第二周："><a href="#第二周：" class="headerlink" title="第二周："></a>第二周：</h3><p>学习如何使用ONNX模型</p><h3 id="第三周："><a href="#第三周：" class="headerlink" title="第三周："></a>第三周：</h3><p>探索WindowsML的性能和内存优化</p><h3 id="第四周："><a href="#第四周：" class="headerlink" title="第四周："></a>第四周：</h3><p>学习如何在执行链中使用多个ML模型</p><h3 id="第五周："><a href="#第五周：" class="headerlink" title="第五周："></a>第五周：</h3><p>掌握图像分类和数据分析的实际操作，包括使用自定义视觉、ML.NET、pytorch以及Windows机器学习进行图像分类和数据分析</p><h3 id="第六周："><a href="#第六周：" class="headerlink" title="第六周："></a>第六周：</h3><p>学习创建基本的WindowsML UMP应用程序，并将训练好的模型转换为ONNX格式</p><h3 id="第七周："><a href="#第七周：" class="headerlink" title="第七周："></a>第七周：</h3><p>使用Windows机器学习部署模型，并导出模型</p><h3 id="第八周："><a href="#第八周：" class="headerlink" title="第八周："></a>第八周：</h3><p>创建WindowsML窗体应用程序</p><h3 id="第九周："><a href="#第九周：" class="headerlink" title="第九周："></a>第九周：</h3><p>汇报课题完毕</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>胸部X射线噪声分类</title>
      <link href="/2024/04/01/%E9%9F%A6%E5%87%A4%E7%BE%BD/"/>
      <url>/2024/04/01/%E9%9F%A6%E5%87%A4%E7%BE%BD/</url>
      
        <content type="html"><![CDATA[<h3 id="课题介绍"><a href="#课题介绍" class="headerlink" title="课题介绍"></a>课题介绍</h3><p>胸部X射线是常用的医疗影像学检查方法之一。然而X图像中可能存在各种噪声，这种噪声会影响图像的质量和诊断的结果。</p><p>因此该课题致力于胸部 X 射线噪声的分类工作。通过对大量胸部 X 射线图像数据的深入分析，精确开发噪声分类算法，以准确识别不同类型的噪声。进一步探究各类噪声对图像质量和诊断结果的具体影响，从而寻求降低噪声干扰的有效方法，为提升诊断准确性提供有力支持。</p><p>BoMD多标签描述符包方法</p><p>本文提出了一种专为有噪声多标签CXR(胸部x射线)学习设计的新方法，该方法可检测并平滑地重新标记数据集中的有噪声样本，以用于普通多标签分类器的训练。该方法通过将图像分割为多个区域并提取每个区域的特征描述符，然后将这些描述符组合成一个全局的特征向量表示图像。这种多标签描述符包的表示方式有助于减少噪声标签的影响，提高分类器的性能和鲁棒性。</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员:"></a>小组成员:</h3><p>韦凤羽2130330038</p><p>周雯星2130330044</p><p>贾明先2130330027</p><h3 id="第一周"><a href="#第一周" class="headerlink" title="第一周:"></a>第一周:</h3><p>1.小组人员广泛搜集胸部X射线和噪声分类领域的相关文献并且对论文有了一定的了解。</p><p>2.小组成员掌握了常见的图像处理方法和噪声分类方法。学习了python等相关编程的基础。</p><p>3.代码:小组成员安装和配置所需的软件和硬件设备，导入包和安装项目依赖。</p><p>4.正在尝试数据集的导入。</p><p>汇报人：贾明先</p><h3 id="第二周"><a href="#第二周" class="headerlink" title="第二周:"></a>第二周:</h3><p>l 进行了广泛的文献调研，总结了当前处理噪声标签和医学图像分类问题的主流方法。</p><p>l 收集了NIH胸部X光的数据集，并进行了初步的数据清洗和预处理工作。</p><p>l 分析了数据集中的噪声标签情况，明确了噪声来源和表现形式。</p><h3 id="第三周"><a href="#第三周" class="headerlink" title="第三周:"></a>第三周:</h3><ol><li><p>设计了BoMD方法的基本框架，包括图像描述学习阶段和图构建阶段。</p></li><li><p>实现了图像描述学习阶段的算法，将训练图像转换为视觉描述符，并映射到语义空间。</p></li><li><p>进行了初步的算法测试，验证了图像描述学习阶段的有效性。</p></li></ol><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周:"></a>第四周:</h3><p>l 完成了图构建阶段的算法实现，每个图像由学习得到的视觉描述符组成的子图表示。</p><p>l 设计了平滑重标记策略，用于处理带有噪声的多标签图像。</p><p>l 在小规模的测试集上进行了初步的实验，验证了图构建和噪声平滑重标记的效果。</p><h3 id="第五周"><a href="#第五周" class="headerlink" title="第五周:"></a>第五周:</h3><p>   1．在NIH胸部X光数据集上进行了全面的实验，比较了BoMD方法与基准卷积神经网络模型的性能。</p><p>   2．评估了分类准确率和F1得分等关键指标，分析了BoMD方法在处理含噪声的多标签医学影像分类任务中的优势。</p><ol start="3"><li>记录了实验结果，并进行了初步的分析和讨论。</li></ol><h3 id="第六周"><a href="#第六周" class="headerlink" title="第六周:"></a>第六周:</h3><p>l 对实验结果进行了深入分析，找出了影响分类性能的关键因素。</p><p>l 根据分析结果，对BoMD方法进行了优化和改进，包括调整模型参数、改进图像描述学习算法等。</p><p>l 在优化后的模型上进行了实验验证，验证了优化效果。</p><h3 id="第七周"><a href="#第七周" class="headerlink" title="第七周:"></a>第七周:</h3><ol><li><p>将BoMD方法扩展到其他胸部X射线数据集上，如PadChest和Chest X-ray 14数据集。</p></li><li><p>在这些数据集上进行了实验，评估了BoMD方法的泛化能力和鲁棒性。</p></li><li><p>对实验结果进行了汇总和分析，得出了最终的结论。</p></li></ol><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周:"></a>第八周:</h3><p>l 根据实验结果和分析，写了项目论文的初稿。</p><p>l 对论文进行了多次修改和完善，确保内容准确、逻辑清晰、表达流畅。</p><p>l 完成了论文的图表制作和格式调整工作。</p><h3 id="第九周"><a href="#第九周" class="headerlink" title="第九周:"></a>第九周:</h3><p>1．对整个项目进行了总结，包括项目的目标、方法、实验结果和结论等。</p><p>2．在项目汇报会上进行了项目展示。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>异常检测</title>
      <link href="/2024/04/01/%E9%83%AD%E5%BA%86/"/>
      <url>/2024/04/01/%E9%83%AD%E5%BA%86/</url>
      
        <content type="html"><![CDATA[<p>MIST-VAD aims to combine the multiple instance Self-training framework with video anomaly detection, which is to find anomalous events spatially or temporarily in one video.</p><h3 id="小组成员："><a href="#小组成员：" class="headerlink" title="小组成员："></a>小组成员：</h3><p>郭庆 2130330022</p><h3 id="第一周"><a href="#第一周" class="headerlink" title="第一周"></a>第一周</h3><p>完成谷歌账号注册，了解免费gpu的使用</p><p><img src="/2024/04/01/%E9%83%AD%E5%BA%86/image-20240409103124794.png" alt="image-20240409103124794"></p><h3 id="第二周"><a href="#第二周" class="headerlink" title="第二周"></a>第二周</h3><p>搞定环境配置，项目框架大致了解，补充深度学习知识</p><img src="/2024/04/01/%E9%83%AD%E5%BA%86/image-20240409103211021.png" alt="image-20240409103211021" style="zoom:50%;"><p><img src="/2024/04/01/%E9%83%AD%E5%BA%86/image-20240409103232872.png" alt="image-20240409103232872"></p><p><img src="/2024/04/01/%E9%83%AD%E5%BA%86/image-20240409103245150.png" alt="image-20240409103245150"></p><h3 id="第三周"><a href="#第三周" class="headerlink" title="第三周"></a>第三周</h3><p>搞定环境配置，项目框架大致了解，补充深度学习知识</p><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><p> 程序训练过程中遇到问题，尝试多次上传数据集来解决</p><h3 id="第五周"><a href="#第五周" class="headerlink" title="第五周"></a>第五周</h3><p>与同课题的同学讨论实验过程的细节，以及对于训练结果帧率AUC等的看法</p><h3 id="第六周"><a href="#第六周" class="headerlink" title="第六周"></a>第六周</h3><p>明确结课标准，学习visio画图，以及英文实验报告的撰写等</p><p><img src="/2024/04/01/%E9%83%AD%E5%BA%86/image-20240622101938361.png" alt="image-20240622101938361"></p><h3 id="第七周"><a href="#第七周" class="headerlink" title="第七周"></a>第七周</h3><p>回顾与整理课题设计相关步骤和过程，以及过程中的所学内容</p><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周"></a>第八周</h3><p>再次研读课题论文，着手写课题实验报告</p><p><img src="/2024/04/01/%E9%83%AD%E5%BA%86/image-20240622101949752.png" alt="image-20240622101949752"></p><h3 id="第九周"><a href="#第九周" class="headerlink" title="第九周"></a>第九周</h3><p> 汇报课题完成</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>神经辐射场（Neural Radiance Field，简称 NeRF）</title>
      <link href="/2024/04/01/%E4%BA%8E%E5%8F%AF%E6%99%B4/"/>
      <url>/2024/04/01/%E4%BA%8E%E5%8F%AF%E6%99%B4/</url>
      
        <content type="html"><![CDATA[<p>项目介绍：<br>神经辐射场（Neural Radiance Field，简称 NeRF）是一种神经网络，可以从部分二维图像集重建复杂的三维场景。各种模拟、游戏、媒体和物联网 (IoT) 应用都需要三维图像，以使数字交互更加真实和准确。 NeRF 学习特定场景的场景几何形状、对象和角度，然后从新的视角渲染逼真的 3D 视图，自动生成合成数据来填补空白。它作为一种具有隐式场景表示的新型视场合成技术，在计算机视觉领域引起了广泛的关注。作为一种新颖的视图合成和三维重建方法，NeRF 模型在机器人、城市地图、自主导航、虚拟现实&#x2F;增强现实等领域都有广泛的应用。</p><h3 id="小组成员："><a href="#小组成员：" class="headerlink" title="小组成员："></a>小组成员：</h3><p>于可晴 2130330040<br>卢熙哲 2130330035</p><h2 id="项目进度："><a href="#项目进度：" class="headerlink" title="项目进度："></a>项目进度：</h2><h3 id="第一到七周："><a href="#第一到七周：" class="headerlink" title="第一到七周："></a>第一到七周：</h3><p>源代码模型已经跑通，官方数据集fern和flower都已经运行出结果。<br>目前正在做自己的数据集。</p><h3 id="第八周："><a href="#第八周：" class="headerlink" title="第八周："></a>第八周：</h3><p>已经自己制作出来了数据集，并且跑通了结果，目前在写实验报告</p><h3 id="第九周："><a href="#第九周：" class="headerlink" title="第九周："></a>第九周：</h3><p>汇报课题完毕</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>足球比分预测3</title>
      <link href="/2024/04/01/%E5%B7%B4%E5%9B%BE%E9%82%A3%E9%A1%BA/"/>
      <url>/2024/04/01/%E5%B7%B4%E5%9B%BE%E9%82%A3%E9%A1%BA/</url>
      
        <content type="html"><![CDATA[<p>本系统通过使用深度学习来预测足球比赛的结果，从而最大化收益。首先，我们将使用一个包含全连接层（Dense layers）和一些基础数据的简单模型。然后，我们将尝试使用长短时记忆网络（LSTM）层和序列输入数据来改善结果。这样一些非常具有创新性的尝试被用于预测体育赛事。这个方法将重点关注使用更加传统的比赛统计数据，因为这类数据通常更容易获取，并且可以在更大规模的数据集上进行训练。通过使用深度学习，我们可以捕获数据中的复杂模式，并尝试预测比赛结果。</p><p>具体来说，使用Dense层的简单模型可能是一个好的起点，但这样的模型可能无法捕获数据中的时间序列依赖性。相比之下，LSTM层可以处理序列数据，更好地建模比赛过程中的动态变化。例如，一个队伍在前半场的表现可能会影响后半场的结果，而LSTM能够捕捉这种时间上的依赖关系。</p><p>为了进一步提高预测准确性，我们还可以考虑以下几点：</p><ol><li><p>特征工程：选择和创建与比赛结果高度相关的特征是非常重要的。这可能包括队伍的历史表现、球员状态、战术安排等。</p></li><li><p>数据预处理：由于比赛数据的来源可能不同，数据预处理对于消除噪声和不一致性非常关键。</p></li><li><p>模型优化：通过调整模型参数（如学习率、批次大小、层数等）以及使用正则化技术来防止过拟合，可以优化模型的性能。</p></li><li><p>集成学习：将多个模型的预测结果结合起来，可以进一步提高预测的准确性。</p></li><li><p>实时更新：考虑到体育比赛数据的动态变化，一个能够实时更新并重新训练模型的系统可能会更有效。</p></li></ol><p>需要注意的是，尽管深度学习在预测方面有很大的潜力，但体育比赛的结果仍然受到许多不可预测因素的影响，如伤病、天气、裁判决策等。因此，即使是最先进的模型也只能提供概率性的预测，而不是确定性的结果。</p><h3 id="小组成员："><a href="#小组成员：" class="headerlink" title="小组成员："></a>小组成员：</h3><p>巴图那顺 2130330002</p><h3 id="进度："><a href="#进度：" class="headerlink" title="进度："></a>进度：</h3><ol><li><p>理解代码结构：首先，需要对LSTM和全连接神经网络的基本原理有深入的理解。LSTM是一种特殊的循环神经网络（RNN），适合处理时间序列数据，具有独特的记忆机制和动态特性。全连接神经网络则是每个节点都与其他节点相连的神经网络，通常用于分类和回归任务。在Keras中，可以将CNN和LSTM结合起来构建混合模型。具体来说，CNN层用于特征提取，而LSTM层则用于捕捉时间序列中的长期依赖关系，最后通过全连接层进行输出。</p></li><li><p>代码实现与结构解析：代码实现方面，首先需要导入必要的库，如TensorFlow或PyTorch，并生成模拟数据用于训练模型。接着，定义一个包含LSTM层和全连接层的模型结构。例如，在Keras中，可以使用Sequential模型来定义这种结构。在编译模型时，选择合适的优化器和损失函数。常用的优化器有Adam，损失函数则可以选择分类交叉熵损失函数。</p></li><li><p>跑通代码并验证模型效果：在完成模型定义后，需要进行数据预处理、模型训练和验证等步骤。首先，将数据集分为训练集和测试集，然后使用训练集进行模型训练。在训练过程中，监控模型的损失和准确率，确保模型能够有效学习。训练完成后，使用测试集评估模型的性能。可以通过计算模型在测试集上的准确率、召回率等指标来验证模型的效果。</p></li><li><p>总结与展望：最后，在汇报中总结当前的工作进展，并提出未来的研究方向。可以考虑进一步优化模型结构，尝试不同的数据预处理方法，或者结合更多的特征来提高预测的准确性。**</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>足球比分预测2</title>
      <link href="/2024/04/01/%E7%8E%8B%E5%BF%A0%E6%BA%90/"/>
      <url>/2024/04/01/%E7%8E%8B%E5%BF%A0%E6%BA%90/</url>
      
        <content type="html"><![CDATA[<p>在预测体育赛事方面，人们进行了一些真正有创意的尝试。这个项目通过使用深度学习预测足球比赛的结果来实现收益最大化。我们将使用一个带有密集层和一些基本数据的简单模型。使用 LSTM 层和连续输入数据来改进结果。之后使用Bidirectional GRU模型进行改进。</p><h3 id="小组成员："><a href="#小组成员：" class="headerlink" title="小组成员："></a>小组成员：</h3><p>王忠源 2130330014</p><p>田丰泰 2130330011</p><h3 id="第一到四周："><a href="#第一到四周：" class="headerlink" title="第一到四周："></a>第一到四周：</h3><p>下载相关代码，配置运行环境，读懂代码，将代码运行成功，复现结果。</p><h3 id="第五到八周："><a href="#第五到八周：" class="headerlink" title="第五到八周："></a>第五到八周：</h3><p>构想改进方法，尝试更改模型，改写代码，并尝试运行。</p><h3 id="第九到十二周："><a href="#第九到十二周：" class="headerlink" title="第九到十二周："></a>第九到十二周：</h3><p>运行更改后的代码，得出结果，撰写报告。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>足球比分预测1</title>
      <link href="/2024/04/01/%E5%86%AF%E5%AE%97%E7%91%9E/"/>
      <url>/2024/04/01/%E5%86%AF%E5%AE%97%E7%91%9E/</url>
      
        <content type="html"><![CDATA[<p>该内核试图通过使用深度学习预测结果来最大化足球比赛的回报。首先，我们将使用一个简单的模型与密集层和一些基本数据。然后，我们将尝试使用LSTM层和顺序输入数据来改善结果。一些真正有创造性的尝试已被用于预测体育赛事。这种方法将侧重于使用更传统的比赛统计数据，如进球和得分。在这一步中，项目对数据进行了初步分析，以了解数据的特征和模式。具体发现包括：比赛赔率对预测进球数和进球差有一定的预测能力。之前比赛的平均进球数也对预测下一场比赛的进球数有一定的预测能力，并且与赔率有很强的相关性。</p><h3 id="小组成员："><a href="#小组成员：" class="headerlink" title="小组成员："></a>小组成员：</h3><p>冯宗睿  2130330005</p><p>卢孝忠  2130330008</p><h3 id="第一周："><a href="#第一周：" class="headerlink" title="第一周："></a>第一周：</h3><p>通过阅读原代码了解其原理和具体步骤代码，读懂具体的关于预测的代码及其含义。数据库使用VSCode可以显著提高开发效率和代码质量。代码的管理和编辑，获取用于训练和测试的足球比赛数据。这些数据包括历史比赛结果、球队信息、球员数据等。从开放数据源、网站获取这些数据，并将其存储为适合模型训练的格式，比如CSV文件、数据库表格等</p><h3 id="第二周："><a href="#第二周：" class="headerlink" title="第二周："></a>第二周：</h3><p>vscode跑通代码使其可正常运行无错误，配置具体环境，在VSCode中创建一个新的项目或者文件夹，确保已经安装了所需的Python环境以及必要的库。训练集的代码，模型构建，根据预测需求和数据特征，选择合适的深度学习模型。使用多层感知器（MLP）、循环神经网络（RNN）或者长短期记忆网络（LSTM）等模型。在VSCode中，创建一个Python脚本文件来定义和构建模型。</p><h3 id="第三周："><a href="#第三周：" class="headerlink" title="第三周："></a>第三周：</h3><p>预测集使用准备好的数据集来训练你的模型。在VSCode中，你可以编写训练脚本来加载数据、定义训练过程、选择优化器和损失函数，并监控训练过程中的性能指标。你可以利用VSCode的调试功能来调试和优化你的训练代码。训练完成后，你需要对模型进行评估和验证，以确保其性能达到预期。你可以使用测试集来评估模型的泛化能力，并根据评估结果对模型进行调整和优化。</p><h3 id="第四周："><a href="#第四周：" class="headerlink" title="第四周："></a>第四周：</h3><p>R语言绘图代码</p><h3 id="第五到八周："><a href="#第五到八周：" class="headerlink" title="第五到八周："></a>第五到八周：</h3><p>跑通代码配置环境，利用VSCode的调试功能来调试和优化你的训练代码。训练完成后，对模型进行评估和验证，以确保其性能达到预期。你可以使用测试集来评估模型的泛化能力。</p><h3 id="第八到十二周："><a href="#第八到十二周：" class="headerlink" title="第八到十二周："></a>第八到十二周：</h3><p>修改代码实现其功能</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>人体姿态估计</title>
      <link href="/2024/04/01/%E9%AB%98%E4%BA%9A%E8%8C%B9/"/>
      <url>/2024/04/01/%E9%AB%98%E4%BA%9A%E8%8C%B9/</url>
      
        <content type="html"><![CDATA[<p>基于OpenPose算法的人体姿态估计技术。OpenPose 是基于卷积神经网络和监督学习并以caffe为框架开发的人体姿态识别项目，是世界上第一个在单个图像上联合检测人体、手、面部和脚关键点（总共135个关键点）的实时多人检测算法，作为一种先进的实时多人姿态估计库，它通过深度学习和计算机视觉技术，能够精确地检测和跟踪图像或视频中的人体关键点，进而估计人体的姿态。该技术不仅适用于单人姿态估计，更能应对多人场景，实现复杂环境下的实时姿态分析。</p><h3 id="小组成员："><a href="#小组成员：" class="headerlink" title="小组成员："></a>小组成员：</h3><p>高亚茹 2130330019</p><p>黄智强 2130330006</p><h3 id="第一周："><a href="#第一周：" class="headerlink" title="第一周："></a>第一周：</h3><p>学习论文，了解相关原理</p><p><img src="/2024/04/01/%E9%AB%98%E4%BA%9A%E8%8C%B9/image-20240622120053866.png" alt="image-20240622120053866"></p><h3 id="第二周："><a href="#第二周：" class="headerlink" title="第二周："></a>第二周：</h3><p>下载代码，安装环境</p><h3 id="第三周："><a href="#第三周：" class="headerlink" title="第三周："></a>第三周：</h3><p>安装一系列相关的库，比如 OpenCV，open pose，mediapipe，troch</p><p><img src="/2024/04/01/%E9%AB%98%E4%BA%9A%E8%8C%B9/image-20240622120109744.png" alt="image-20240622120109744"></p><h3 id="第四周："><a href="#第四周：" class="headerlink" title="第四周："></a>第四周：</h3><p>下载 COCO 数据集</p><p><img src="/2024/04/01/%E9%AB%98%E4%BA%9A%E8%8C%B9/image-20240622120123237.png" alt="image-20240622120123237"></p><h3 id="第五周："><a href="#第五周：" class="headerlink" title="第五周："></a>第五周：</h3><p>运行 picture.demo</p><p><img src="/2024/04/01/%E9%AB%98%E4%BA%9A%E8%8C%B9/image-20240622120208031.png" alt="image-20240622120208031"></p><h3 id="第六周："><a href="#第六周：" class="headerlink" title="第六周："></a>第六周：</h3><p>运行 train_VGG19.py</p><p><img src="/2024/04/01/%E9%AB%98%E4%BA%9A%E8%8C%B9/image-20240622120157791.png" alt="image-20240622120157791"></p><h3 id="第七周："><a href="#第七周：" class="headerlink" title="第七周："></a>第七周：</h3><p>运行 train_VGG19.py</p><p><img src="/2024/04/01/%E9%AB%98%E4%BA%9A%E8%8C%B9/image-20240622120223527.png" alt="image-20240622120223527"></p><h3 id="第八周："><a href="#第八周：" class="headerlink" title="第八周："></a>第八周：</h3><p>运行train_VGG19.py</p><h3 id="第九周："><a href="#第九周：" class="headerlink" title="第九周："></a>第九周：</h3><p>写实验报告</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ROSEMASTERX3 PLUS机器小车</title>
      <link href="/2024/04/01/%E5%B4%94%E5%B2%A9/"/>
      <url>/2024/04/01/%E5%B4%94%E5%B2%A9/</url>
      
        <content type="html"><![CDATA[<p>ROSMASTERX3 PLUS是一款基于ROS机器人操作系统开发的麦轮全向移动机器人，它支持Jetson系列主板和树莓派4B作为主控，并搭载了激光雷达、深度相机、6自由度机械臂等高性能硬件配置，可实现建图导航、自动驾驶、人体特征识别、movelt机械臂仿真控制等应用。</p><p>功能：具备自主巡航、避障、循迹、遥控等多种功能，支持多种编程语言和开发环境，方便用户进行二次开发和定制。</p><p>特点：该车采用模块化设计，易于组装和拆卸；拥有丰富的扩展接口，支持多种传感器和执行器的接入；配备高性能处理器和精密传感器，确保小车运行稳定、反应灵敏。</p><p>应用场景：适用于学校教育、机器人比赛、科研实验、创意DIY等多种场景。它可以帮助学生学习编程和机器人技术，提高创新能力和动手能力；也可以为科研人员提供便捷的实验平台，促进科技成果转化。</p><h3 id="小组成员："><a href="#小组成员：" class="headerlink" title="小组成员："></a>小组成员：</h3><p>崔延     2130330003</p><p>葛乐力 2130330020</p><h2 id="课题进度："><a href="#课题进度：" class="headerlink" title="课题进度："></a>课题进度：</h2><h3 id="已经初步完成，后续修改完善"><a href="#已经初步完成，后续修改完善" class="headerlink" title="已经初步完成，后续修改完善"></a>已经初步完成，后续修改完善</h3><p><img src="/2024/04/01/%E5%B4%94%E5%B2%A9/image-20240409110510046.png" alt="image-20240409110510046"></p><img src="/2024/04/01/%E5%B4%94%E5%B2%A9/image-20240409110550441.png" alt="image-20240409110550441" style="zoom: 50%;"><p><img src="/2024/04/01/%E5%B4%94%E5%B2%A9/image-20240409110337618.png" alt="image-20240409110337618"></p><p><img src="/2024/04/01/%E5%B4%94%E5%B2%A9/image-20240409110344389.png" alt="image-20240409110344389"></p><p><img src="/2024/04/01/%E5%B4%94%E5%B2%A9/image-20240409110358489.png" alt="image-20240409110358489"></p><p><img src="/2024/04/01/%E5%B4%94%E5%B2%A9/image-20240409110410415.png" alt="image-20240409110410415"></p><p>ROS节点来处理激光雷达数据，并设置了各种参数，如串口、雷达类型、数据范围</p><p><img src="/2024/04/01/%E5%B4%94%E5%B2%A9/image-20240409110450613.png" alt="image-20240409110450613"></p><p><img src="/2024/04/01/%E5%B4%94%E5%B2%A9/image-20240409110642097.png" alt="image-20240409110642097"></p><p>汇报人：崔延</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>掩模训练下的图像去噪</title>
      <link href="/2024/04/01/%E8%B5%B5%E5%A4%A9%E5%A5%87/"/>
      <url>/2024/04/01/%E8%B5%B5%E5%A4%A9%E5%A5%87/</url>
      
        <content type="html"><![CDATA[<p>我们小组的课程设计主题是掩模训练下的图像去噪。基于深度学习的方法往往存在泛化能力不足的问题，为了增强去噪网络的泛化性能，以掩模训练的方法涉及屏蔽输入图像的随机像素，并在训练期间重建缺失的信息，此方法比其他深度学习模型表现出更好的泛化能力，并且直接适用于现实世界的场景。由于该技术目前已较为成熟，我们小组的实验任务为理解论文和代码，理解该技术思想以及跑通代码，进行实验。</p><h3 id="小组成员："><a href="#小组成员：" class="headerlink" title="小组成员："></a>小组成员：</h3><p>赵天奇 2130330043</p><p>马桂雪 2130330037</p><p>贾月华 2130330028</p><h2 id="课题进度"><a href="#课题进度" class="headerlink" title="课题进度"></a>课题进度</h2><h3 id="第一周："><a href="#第一周：" class="headerlink" title="第一周："></a>第一周：</h3><p>发现问题向老师请教后，解决大部分问题，目前仍有一个问题在解决中</p><h3 id="第二周："><a href="#第二周：" class="headerlink" title="第二周："></a>第二周：</h3><p>学习与所发现问题的相关知识</p><h3 id="第三周："><a href="#第三周：" class="headerlink" title="第三周："></a>第三周：</h3><p>代码已跑通，接下来的任务看懂代码，明白代码的意义</p><h3 id="第四周："><a href="#第四周：" class="headerlink" title="第四周："></a>第四周：</h3><p>代码已跑通，接下来的任务看懂代码，明白代码的意义</p><h3 id="第五周："><a href="#第五周：" class="headerlink" title="第五周："></a>第五周：</h3><p>代码已跑通，接下来的任务看懂代码，明白代码的意义</p><h3 id="第六周："><a href="#第六周：" class="headerlink" title="第六周："></a>第六周：</h3><p>代码已跑通，接下来的任务看懂代码，明白代码的意义</p><h3 id="第七周："><a href="#第七周：" class="headerlink" title="第七周："></a>第七周：</h3><p>写实验报告</p><h3 id="第八周："><a href="#第八周：" class="headerlink" title="第八周："></a>第八周：</h3><p>完善实验报告</p><h3 id="第九周："><a href="#第九周：" class="headerlink" title="第九周："></a>第九周：</h3><p>进度汇报</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
